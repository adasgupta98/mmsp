% MMSP.chapter7.tex

\chapter{Advanced \MMSP\ Examples}\label{ch:advanced}

\section{Cahn-Hilliard}
	\emph{This section describes the mathematical foundations of the example code in }
	{\tt MMSP/examples/phase\_transitions/cahn\_hilliard/convex\_splitting}.

	Spinodal decomposition is readily modeled using the Helmholtz free energy functional
	\[\mathcal{F} = \int\left[F(C) + \kappa|\nabla C|^2\right]\mathrm{d}V,\]
	with free energy density
	\[F(C) = \frac{C^4}{4} - \frac{C^2}{2} + \kappa|\nabla C|^2.\]
	The first derivative
	\[f(C) = \frac{\mathrm{d}F}{\mathrm{d}C} = C^3 - C - \kappa\nabla^2 C\]
	has three terms, two of which are convex in their arguments $\left(\left.\frac{\mathrm{d}(C^3 - \kappa\nabla^2 C)}{\mathrm{d}C}\right|_{C\geq0}\geq0\right)$
	and the other concave $\left(\frac{\mathrm{d}(-C)}{\mathrm{d}C}<0\right)$.
	From a numerical perspective, the convex term tends to smooth perturbations in the solution, and is called contractive, while the concave term
	amplifies perturbations, and is expansive.
	When used in the Cahn-Hilliard equation of motion,
	\[\frac{\partial C}{\partial t} = D\nabla^2\left[C^3 - C - \kappa\nabla^2C\right],\]
	improperly discretized equations of motion may increase the system energy, which is not physical.
	
	\subsection{Convex splitting}
	Convex splitting aims to guarantee energy stability $\left(\frac{\Delta\mathcal{F}}{\Delta t}\leq 0\right)$
	by discretizing the equation semi-implicitly, evaluating contractive terms at the next timestep (index $n+1$)
	and expansive terms at the old timestep (index $n$):
	\begin{align*}
		f_c(C) &= C^3 - \kappa\Delta_hC\\
		f_e(C) &= -C\\
		C^{n+1} &= C^n + kD\Delta_h\mu^{n+1}\\
		\mu^{n+1} &= f_c(C^{n+1}) + f_e(C^n),
	\end{align*}
	with common symbolic substitutions for timestep $(k=\Delta t)$ and discrete Laplacian operator $(\Delta_h)$
	with grid spacing $h=\Delta x=\Delta y$. Collecting old and new terms,
	\begin{align}
	\begin{split}
		C^{n+1} - kD\Delta_h\mu^{n+1} &= C^n\\
		\kappa\Delta_hC^{n+1} - f_c(C^{n+1}) + \mu^{n+1} &= f_e(C^n).
	\end{split}
	\label{eqn:convexeom}
	\end{align}
	
	\subsection{Semi-implicit discretization}
	Let's prepare to solve the system of equations iteratively.
	In the Jacobi method, elements along the matrix diagonal (i.e., the grid point of interest) are evaluated at
	the next guess (index $l+1$), and off-diagonal elements belong to the last guess (index $l$).
	The nonlinear source term $\left(C_{l+1}^{n+1}\right)^3$ is linearized so that
	\begin{align*}
		\ell_c(C) &= \left(C_l^{n+1}\right)^2\\
		f_c(C) &= \ell_c(C_l^{n+1})C_{l+1}^{n+1} - \kappa\Delta_hC_{l+1}^{n+1}.
	\end{align*}
	Let's separate the discrete Laplacian $\Delta_h$ into its central (matrix-diagonal) and fringe (off-diagonal) components. The $d$-dimensional
	formula is readily generalized from the 5-point stencil in 2-dimensions given $\Delta x=\Delta y=h$:
	\begin{align*}
		\Delta_hC(x,y)	&= \frac{C(x+1,y)-2C(x,y)+C(x-1,y)+C(x,y+1)-2C(x,y)+C(x,y-1)}{h^2}\\
						&= \frac{-4C(x,y)}{h^2}+\frac{C(x+1,y)+C(x-1,y)+C(x,y+1)+C(x,y-1)}{h^2}\\
		\Delta_hC(\vec{x})	&= \frac{-2dC(\vec{x})}{h^2} + \Delta_\circ C(\vec{x}),
	\end{align*}
	with the ``fringe Laplacian" operator $\Delta_\circ$ introduced as convenient shorthand.

	Finally, we have the semi-implicit discretization properly indexed for iteration:
	\begin{align}
	\begin{split}
		C_{l+1}^{n+1} + \frac{2dkD}{h^2}\mu_{l+1}^{n+1} &= C^n + kD\Delta_\circ\mu_l^{n+1}\\
		-\frac{2d\kappa}{h^2}C_{l+1}^{n+1} - \ell_c(C_l^{n+1})C_{l+1}^{n+1} + \mu_{l+1}^{n+1} &= f_e(C^n) - \kappa\Delta_\circ C_l^{n+1},
	\end{split}
	\label{eqn:jacobieom}
	\end{align}
	or in matrix form
	\[
		\begin{bmatrix}
			1	&	\frac{2dkD}{h^2}\\[0.5em]
			-\frac{2d\kappa}{h^2} - \ell_c\left(C_l^{n+1}\right) & 1\\
		\end{bmatrix}
		\begin{bmatrix}
			C_{l+1}^{n+1} \\[0.5em]
			\mu_{l+1}^{n+1}\\
		\end{bmatrix}
		=
		\begin{bmatrix}
			C^n + kD\Delta_\circ\mu_l^{n+1}\\[0.5em]
			f_e(C^n) - \kappa\Delta_\circ C_l^{n+1}\\
		\end{bmatrix}.
	\]

	This system of linear equations is solved using Cramer's rule for $2\times 2$ systems $\mathbf{A}\mathrm{x}=\mathrm{b}$:
	\[
		x_1 = \frac{b_1a_{22} - b_2a_{12}}{a_{11}a_{22} - a_{21}a_{12}}, \
		\hspace{1em}x_2 = \frac{a_{11}b_2 - a_{21}b_1}{a_{11}a_{22} - a_{21}a_{12}}.
	\]
	For clarity, the code implementing Cramer's rule for the equations of motion retains the $a_{ij}$, $x_i$, $b_i$ notation.		

	\subsection{Jacobi iteration}
	To determine values for the next timestep $C^{n+1}$ and $\mu^{n+1}$, seed the matrix using the old values $C^n$ and $\mu^n$,
	then iterate until the system of equations is true, meaning the error
	\[\eta = ||\mathrm{b}' - \left(\mathbf{A}\mathrm{x}\right)'||,\]
	using the $L_2$ vector norm $||\cdot ||$, falls tolerably close to zero.
	The matrix terms are dashed $(')$ for a reason:
	the residual does not depend on the iteration scheme chosen, so $\left(\mathbf{A}\mathrm{x}\right)'=\mathrm{b}'$ 
	represents the equations of motion prior to $l$-indexing for Jacobi iteration, Eqn.~\ref{eqn:convexeom} rather than Eqn.~\ref{eqn:jacobieom}:
	\begin{align*}
		\begin{split}
			\mathbf{A}\mathrm{x} &=
			\begin{bmatrix}
				C_{l+1}^{n+1} + \frac{2dkD}{h^2}\mu_{l+1}^{n+1}\\[0.5em]
				-\frac{2d\kappa}{h^2}C_{l+1}^{n+1} - \ell_c(C_l^{n+1})C_{l+1}^{n+1} + \mu_{l+1}^{n+1}\\
			\end{bmatrix}\\
			\mathrm{b} &=
			\begin{bmatrix}
				C^n + kD\Delta_\circ\mu_l^{n+1}\\[0.5em]
				f_e(C^n) - \kappa\Delta_\circ C_l^{n+1}\\
			\end{bmatrix}
		\end{split},
		&
		\begin{split}
			\left(\mathbf{A}\mathrm{x}\right)' &=
			\begin{bmatrix}
				C_{l+1}^{n+1} - kD\Delta_h\mu_{l+1}^{n+1}\\[0.5em]
				        \kappa\Delta_hC_{l+1}^{n+1} - f_c(C_{l+1}^{n+1}) + \mu_{l+1}^{n+1}\\
			\end{bmatrix}\\
			\mathrm{b}' &=
			\begin{bmatrix}
				C^n\\[0.5em]
				f_e(C^n)\\
			\end{bmatrix}
		\end{split}.
	\end{align*}
	Note that $\left(\mathbf{A}\mathrm{x}\right)'=\mathrm{b}'$ depends only on the old \emph{value} and the latest \emph{guess}.
	
	Since the values at $n+1$ are interdependent, the local errors are gathered up in the residual:
	\[r = \frac{1}{2N}\sqrt{\sum\limits_{N}\frac{||\mathrm{b}' - \left(\mathbf{A}\mathrm{x}\right)'||}{||\mathrm{b}'||}},\]
	in which $N$ is the number of nodes in the grid and $2$ is the number of equations being solved at each node.
	Calculating the residual is as expensive as a full iteration, and is done at regular intervals rather than after each iteration.
	An appropriate tolerance for convergence is $r_{tol}\leq10^{-14}$: if $r>r_{tol}$, guess again.

	\subsection{Gauss-Seidel iteration}
	Jacobi iteration requires allocating an extra grid to store intermediate guesses.
	In the interests of speed and memory footprint, red-black Gauss-Seidel is a better choice.
	In this scheme, the grid points are labeled according to a checkerboard pattern with alternating ``red" and ``black" tiles.
	The new guess is calculated on each red tile first, using the previous guess stored in adjacent black tiles;
	then the new guess is calculated on each black tiles, using the new guess stored in adjacent red tiles.
	This scheme also enables seamless switching between serial and parallel architectures, since cells adjacent
	across parallel boundaries can never be both red or both black.
	
	Note that the change from Jacobi to Gauss-Seidel modifies the order in which the guesses are made,
	with minimal effect on the values computed.
	
	\subsection{Successive over-relaxation}
	Successive over-relaxation increases the rate of convergence of Gauss-Seidel iteration by weighted averaging
	the outcome of Cramer's rule (representing the new guess) with the last guess.
	Instead of
	\begin{align*}
		C_{l+1}^{n+1} &= x_1\\
		\mu_{l+1}^{n+1} &= x_2,
	\end{align*}
	we record
	\begin{align*}
		C_{l+1}^{n+1} &= \omega x_1 + (1-\omega)C_l^{n+1} \\
		\mu_{l+1}^{n+1} &= \omega x_2 + (1-\omega)\mu_l^{n+1}.
	\end{align*}
	The relaxation parameter $\omega$ must necessarily satisfy $0<\omega<2$ for convergence.
	Successive over-relaxation requires $\omega>1$, and $\omega=1.1$ is recommended for this specific system.
	$\omega=1$ recovers Gauss-Seidel, and $\omega<1$ is \emph{successive under-relaxation}, which is not recommended.

	
	\subsection{Numerical performance}
	This example is intended to show that \MMSP\ can handle numerical schemes far beyond forward Euler.
	Stability analysis of the explicit scheme requires 
	\[\Delta t_\mathrm{E} < \frac{(\Delta x)^4}{16\kappa D}\mathrm{Co},\]
	with Courant-Friedrichs-Lewy number $\mathrm{Co}\ll 1$.
	Taking $\mathrm{Co}=1/4$ and $\kappa=D=1$, we have $\Delta t_\mathrm{E}=1/64$, and
	integration to $t=50,\!000$ (adequate time to reach equilibrium) takes $3.2\times10^6$ explicit steps.
	
	The semi-implicit scheme discussed here is unconditionally numerically stable, but the larger the step, the more iterations are necessary to converge.
	Early on, when the system is rapidly evolving from initial conditions, the convex splitting scheme can be much more
	expensive than forward Euler.
	As the simulation approaches equilibrium, however, convergence is achieved in many fewer iterations per timestep.
	With $\Delta t=1$ and $\omega=1$, the semi-implicit code takes $2.6\times10^6$ iterations in total to reach $t=50,\!000$.
	Assuming one semi-implicit iteration incurs the same computational cost as one explicit step, this means Gauss-Seidel gives us $1.2\times$ speedup.
	Successive over-relaxation with $\omega=1.1$ does a little better, with a $1.4\times$ boost.

	% Include and discuss contour plot of Work vs (time,omega) when available	
